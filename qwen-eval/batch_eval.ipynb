{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8277f628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 12:23:35 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "from vllm import LLM, SamplingParams\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from evaluate import evaluate\n",
    "from utils import set_seed, load_jsonl, save_jsonl, construct_prompt\n",
    "from parser import *\n",
    "from trajectory import *\n",
    "from data_loader import load_data\n",
    "from python_executor import PythonExecutor\n",
    "from model_utils import load_hf_lm_and_tokenizer, generate_completions\n",
    "from math_eval import parse_args, set_seed, setup, main\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d047ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e0a1074",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--data_names\", default=\"gsm8k,math\", type=str)\n",
    "parser.add_argument(\"--data_dir\", default=\"./data\", type=str)\n",
    "parser.add_argument(\"--model_name_or_path\", default=\"gpt-4\", type=str)\n",
    "parser.add_argument(\"--output_dir\", default=\"./output\", type=str)\n",
    "parser.add_argument(\"--prompt_type\", default=\"tool-integrated\", type=str)\n",
    "parser.add_argument(\"--split\", default=\"test\", type=str)\n",
    "parser.add_argument(\"--num_test_sample\", default=-1, type=int)  # -1 for full data\n",
    "parser.add_argument(\"--seed\", default=0, type=int)\n",
    "parser.add_argument(\"--start\", default=0, type=int)\n",
    "parser.add_argument(\"--end\", default=-1, type=int)\n",
    "parser.add_argument(\"--temperature\", default=0, type=float)\n",
    "parser.add_argument(\"--n_sampling\", default=1, type=int)\n",
    "parser.add_argument(\"--top_p\", default=1, type=float)\n",
    "parser.add_argument(\"--max_tokens_per_call\", default=2048, type=int)\n",
    "parser.add_argument(\"--shuffle\", action=\"store_true\")\n",
    "parser.add_argument(\"--use_vllm\", action=\"store_true\")\n",
    "parser.add_argument(\"--save_outputs\", action=\"store_true\")\n",
    "parser.add_argument(\"--overwrite\", action=\"store_true\")\n",
    "parser.add_argument(\"--use_safetensors\", action=\"store_true\")\n",
    "parser.add_argument(\"--num_shots\", type=int, default=0)\n",
    "parser.add_argument(\n",
    "    \"--apply_chat_template\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Apply chat template to prompt.\",\n",
    ")\n",
    "parser.add_argument(\"--pipeline_parallel_size\", type=int, default=1)\n",
    "parser.add_argument(\n",
    "    \"--adapt_few_shot\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Few shot for multiple-choice questions, zero shot for others.\",\n",
    ")\n",
    "args = parser.parse_args(args=[\"--use_vllm\"])\n",
    "args.top_p = (\n",
    "    1 if args.temperature == 0 else args.top_p\n",
    ")  # top_p must be 1 when using greedy sampling (vllm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcafaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 0\n"
     ]
    }
   ],
   "source": [
    "args.model_name_or_path = \"/home/aiops/chenxw/hfmodels/Qwen2.5-Math-7B\"\n",
    "args.data_names = \"math500\"\n",
    "args.output_dir = \"math_eval-cot\"\n",
    "args.split = \"test\"\n",
    "args.prompt_type = \"qwen25-math-cot\"\n",
    "args.seed = 0\n",
    "args.temperature = 0\n",
    "args.n_sampling = 1\n",
    "args.top_p = 1\n",
    "args.start = 0\n",
    "args.end = -1\n",
    "args.save_outputs = True\n",
    "args.apply_chat_template = False\n",
    "args.overwrite = False\n",
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04a1d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.distributed import (destroy_distributed_environment,\n",
    "                              destroy_model_parallel)\n",
    "import contextlib\n",
    "import gc\n",
    "\n",
    "def cleanup():\n",
    "    destroy_model_parallel()\n",
    "    destroy_distributed_environment()\n",
    "    with contextlib.suppress(AssertionError):\n",
    "        torch.distributed.destroy_process_group()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b0debde",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_gpus = 2\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f18506d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del llm2\n",
    "except:\n",
    "    pass\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09ef5ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 12:27:24 [config.py:600] This model supports multiple tasks: {'embed', 'score', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 05-15 12:27:24 [config.py:1600] Defaulting to use mp for distributed inference\n",
      "INFO 05-15 12:27:24 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-15 12:27:25 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='/home/aiops/chenxw/verl/checkpoints/verl_few_shot/Qwen2.5-Math-7B-true_pi1_aime/global_step_115_hf', speculative_config=None, tokenizer='/home/aiops/chenxw/verl/checkpoints/verl_few_shot/Qwen2.5-Math-7B-true_pi1_aime/global_step_115_hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/home/aiops/chenxw/verl/checkpoints/verl_few_shot/Qwen2.5-Math-7B-true_pi1_aime/global_step_115_hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-15 12:27:25 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 05-15 12:27:25 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 10485760, 10, 'psm_eead5a98'), local_subscribe_addr='ipc:///tmp/a6532463-e0f1-4032-b30a-5511f4ee2f3e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 05-15 12:27:27 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fe74986b5d0>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=21158)\u001b[0;0m INFO 05-15 12:27:27 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ad9dc550'), local_subscribe_addr='ipc:///tmp/0c239614-6d3f-4856-9007-c31c9849250f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 05-15 12:27:28 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fe65f185990>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=21176)\u001b[0;0m INFO 05-15 12:27:28 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_010f5fb4'), local_subscribe_addr='ipc:///tmp/83b3f7b2-d860-40f8-8316-92596d3f1a90', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=21158)\u001b[0;0m INFO 05-15 12:27:28 [utils.py:990] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=21158)\u001b[0;0m INFO 05-15 12:27:28 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=21176)\u001b[0;0m INFO 05-15 12:27:28 [utils.py:990] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=21176)\u001b[0;0m INFO 05-15 12:27:28 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=21176)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=21158)\u001b[0;0m INFO 05-15 12:27:29 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/aiops/chenxw/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 05-15 12:27:29 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/aiops/chenxw/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=21158)\u001b[0;0m INFO 05-15 12:27:29 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_bb7e8410'), local_subscribe_addr='ipc:///tmp/70248e91-e3bf-4589-b748-96b38bb15963', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=21176)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=21158)\u001b[0;0m INFO 05-15 12:27:29 [parallel_state.py:957] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "INFO 05-15 12:27:29 [parallel_state.py:957] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=21176)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=21158)\u001b[0;0m INFO 05-15 12:27:29 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 05-15 12:27:29 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=21176)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=21158)\u001b[0;0m INFO 05-15 12:27:29 [gpu_model_runner.py:1258] Starting to load model /home/aiops/chenxw/verl/checkpoints/verl_few_shot/Qwen2.5-Math-7B-true_pi1_aime/global_step_115_hf...\n",
      "INFO 05-15 12:27:29 [gpu_model_runner.py:1258] Starting to load model /home/aiops/chenxw/verl/checkpoints/verl_few_shot/Qwen2.5-Math-7B-true_pi1_aime/global_step_115_hf...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=21176)\u001b[0;0m WARNING 05-15 12:27:29 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=21158)\u001b[0;0m WARNING 05-15 12:27:29 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe387d161554b06b68efe7ec8eef3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=21176)\u001b[0;0m INFO 05-15 12:27:45 [loader.py:447] Loading weights took 16.03 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=21158)\u001b[0;0m INFO 05-15 12:27:45 [loader.py:447] Loading weights took 15.99 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=21176)\u001b[0;0m INFO 05-15 12:27:46 [gpu_model_runner.py:1273] Model loading took 7.1148 GiB and 16.496002 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=21158)\u001b[0;0m INFO 05-15 12:27:46 [gpu_model_runner.py:1273] Model loading took 7.1148 GiB and 16.536179 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=21176)\u001b[0;0m INFO 05-15 12:27:57 [backends.py:416] Using cache directory: /home/aiops/chenxw/.cache/vllm/torch_compile_cache/7be0024083/rank_1_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=21176)\u001b[0;0m INFO 05-15 12:27:57 [backends.py:426] Dynamo bytecode transform time: 10.92 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=21158)\u001b[0;0m INFO 05-15 12:27:57 [backends.py:416] Using cache directory: /home/aiops/chenxw/.cache/vllm/torch_compile_cache/7be0024083/rank_0_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=21158)\u001b[0;0m INFO 05-15 12:27:57 [backends.py:426] Dynamo bytecode transform time: 10.94 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=21176)\u001b[0;0m INFO 05-15 12:27:58 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=21158)\u001b[0;0m INFO 05-15 12:27:58 [backends.py:115] Directly load the compiled graph for shape None from the cache\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=21176)\u001b[0;0m INFO 05-15 12:28:10 [monitor.py:33] torch.compile takes 10.92 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=21158)\u001b[0;0m INFO 05-15 12:28:10 [monitor.py:33] torch.compile takes 10.94 s in total\n",
      "INFO 05-15 12:28:12 [kv_cache_utils.py:578] GPU KV cache size: 784,336 tokens\n",
      "INFO 05-15 12:28:12 [kv_cache_utils.py:581] Maximum concurrency for 4,096 tokens per request: 191.49x\n",
      "INFO 05-15 12:28:12 [kv_cache_utils.py:578] GPU KV cache size: 784,336 tokens\n",
      "INFO 05-15 12:28:12 [kv_cache_utils.py:581] Maximum concurrency for 4,096 tokens per request: 191.49x\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=21176)\u001b[0;0m INFO 05-15 12:28:40 [custom_all_reduce.py:195] Registering 3819 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=21158)\u001b[0;0m INFO 05-15 12:28:44 [custom_all_reduce.py:195] Registering 3819 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=21176)\u001b[0;0m INFO 05-15 12:28:44 [gpu_model_runner.py:1608] Graph capturing finished in 32 secs, took 1.64 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=21158)\u001b[0;0m INFO 05-15 12:28:44 [gpu_model_runner.py:1608] Graph capturing finished in 32 secs, took 1.64 GiB\n",
      "INFO 05-15 12:28:44 [core.py:162] init engine (profile, create kv cache, warmup model) took 58.27 seconds\n",
      "==================================================\n",
      "data: math500  ,remain samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Epoch 0\n",
      "Unsolved samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 500/500 [00:03<00:00, 156.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_samples': 500, 'num_scores': 500, 'timeout_samples': 0, 'empty_samples': 73, 'acc': np.float64(52.4)}\n"
     ]
    }
   ],
   "source": [
    "# for step in range(10, 16, 5):\n",
    "# step = 10\n",
    "\n",
    "args.model_name_or_path = f\"/home/aiops/chenxw/verl/checkpoints/verl_few_shot/Qwen2.5-Math-7B-true_pi1_aime/global_step_115_hf\"\n",
    "# args.model_name_or_path = \"/home/aiops/chenxw/hfmodels/Qwen2.5-Math-7B\"\n",
    "llm2 = LLM(\n",
    "            model=args.model_name_or_path,\n",
    "            tensor_parallel_size=available_gpus // args.pipeline_parallel_size,\n",
    "            pipeline_parallel_size=args.pipeline_parallel_size,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "tokenizer = None\n",
    "if args.apply_chat_template:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.model_name_or_path, trust_remote_code=True\n",
    "    )\n",
    "\n",
    "data_list = args.data_names.split(\",\")\n",
    "results = []\n",
    "for data_name in data_list:\n",
    "    results.append(main(llm2, tokenizer, data_name, args))\n",
    "\n",
    "# ids = []\n",
    "# scores = []\n",
    "\n",
    "# for sample in results[0][1]:\n",
    "#     ids.append(sample[\"idx\"])\n",
    "#     scores.append(sum(sample[\"score\"]) / args.n_sampling)\n",
    "\n",
    "# df[step] = pd.Series(scores, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1b1fdb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6406f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[step] = pd.Series(scores, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6300d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('MATH500Pass1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a046abd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = [11, 22, 33]\n",
    "idx = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "442f09d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[3] = pd.Series(score, idx, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9a990629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      1     2     3\n",
       "1  11.0  11.0  11.0\n",
       "2  22.0  22.0  22.0\n",
       "3  55.0  55.0  33.0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca49c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1.0\n",
       "1      1.0\n",
       "2      1.0\n",
       "3      1.0\n",
       "4      0.0\n",
       "      ... \n",
       "495    1.0\n",
       "496    1.0\n",
       "497    0.0\n",
       "498    0.0\n",
       "499    0.0\n",
       "Length: 500, dtype: float64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(scores, idx, dtype=np.float64, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ddbecf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores5 = scores.copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm083",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
